package com.textaudit.preprocessor;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.textaudit.preprocessor.dto.ProcessingResult;
import com.textaudit.preprocessor.dto.TextFeatures;
import com.textaudit.preprocessor.dto.TokenizationResult;
import com.textaudit.preprocessor.service.TextProcessingService;
import com.textaudit.preprocessor.service.FeatureExtractionService;
import com.textaudit.preprocessor.service.DataAugmentationService;
import com.textaudit.preprocessor.repository.ProcessedTextRepository;
import lombok.extern.slf4j.Slf4j;
import org.junit.jupiter.api.*;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.test.context.SpringBootTest;
import org.springframework.test.context.ActiveProfiles;
import org.springframework.test.context.TestPropertySource;
import org.springframework.transaction.annotation.Transactional;

import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.stream.Collectors;

/**
 * æ•°æ®é¢„å¤„ç†æœåŠ¡ç”Ÿäº§çº§æµ‹è¯•
 * æµ‹è¯•çŸ¥ä¹æ•°æ®çš„æ¸…æ´—ã€å»é‡ã€åˆ†è¯ã€TF-IDFå‘é‡åŒ–ç­‰åŠŸèƒ½
 */
@Slf4j
@SpringBootTest
@ActiveProfiles("test")
@TestPropertySource(locations = "classpath:application-test.yml")
@TestMethodOrder(MethodOrderer.OrderAnnotation.class)
public class ProductionDataPreprocessorTest {

    @Autowired
    private TextProcessingService textProcessingService;

    @Autowired
    private FeatureExtractionService featureExtractionService;

    @Autowired
    private DataAugmentationService dataAugmentationService;

    @Autowired
    private ProcessedTextRepository processedTextRepository;

    @Autowired
    private ObjectMapper objectMapper;

    private static final String TEST_DATA_DIR = "test-data/preprocessor";
    private static final String REPORT_DIR = "test-reports/preprocessor";
    
    // æµ‹è¯•æ•°æ®é›†
    private List<ZhihuTestData> testDataSet;
    private TestMetrics testMetrics;

    @BeforeAll
    static void setupTestEnvironment() throws IOException {
        // åˆ›å»ºæµ‹è¯•ç›®å½•
        Files.createDirectories(Paths.get(TEST_DATA_DIR));
        Files.createDirectories(Paths.get(REPORT_DIR));
        log.info("æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ");
    }

    @BeforeEach
    void setUp() {
        testMetrics = new TestMetrics();
        testDataSet = generateZhihuTestData();
        log.info("å‡†å¤‡æµ‹è¯•æ•°æ®é›†ï¼Œå…± {} æ¡è®°å½•", testDataSet.size());
    }

    /**
     * æµ‹è¯•1ï¼šæ–‡æœ¬æ¸…æ´—åŠŸèƒ½
     */
    @Test
    @Order(1)
    void testTextCleaning() {
        log.info("å¼€å§‹æµ‹è¯•æ–‡æœ¬æ¸…æ´—åŠŸèƒ½...");
        
        List<CleaningTestResult> results = new ArrayList<>();
        
        for (ZhihuTestData data : testDataSet) {
            long startTime = System.currentTimeMillis();
            
            try {
                String cleanedText = textProcessingService.cleanText(data.getRawContent());
                long processingTime = System.currentTimeMillis() - startTime;
                
                CleaningTestResult result = CleaningTestResult.builder()
                    .originalText(data.getRawContent())
                    .cleanedText(cleanedText)
                    .originalLength(data.getRawContent().length())
                    .cleanedLength(cleanedText.length())
                    .processingTimeMs(processingTime)
                    .htmlTagsRemoved(countHtmlTags(data.getRawContent()) - countHtmlTags(cleanedText))
                    .urlsRemoved(countUrls(data.getRawContent()) - countUrls(cleanedText))
                    .specialCharsRemoved(countSpecialChars(data.getRawContent()) - countSpecialChars(cleanedText))
                    .success(true)
                    .build();
                
                results.add(result);
                testMetrics.incrementSuccessCount();
                
            } catch (Exception e) {
                log.error("æ–‡æœ¬æ¸…æ´—å¤±è´¥: {}", e.getMessage());
                testMetrics.incrementFailureCount();
                
                results.add(CleaningTestResult.builder()
                    .originalText(data.getRawContent())
                    .success(false)
                    .errorMessage(e.getMessage())
                    .build());
            }
        }
        
        // ç”Ÿæˆæ¸…æ´—æµ‹è¯•æŠ¥å‘Š
        generateCleaningReport(results);
        
        // éªŒè¯æ¸…æ´—æ•ˆæœ
        double successRate = (double) testMetrics.getSuccessCount() / testDataSet.size();
        Assertions.assertTrue(successRate >= 0.95, "æ–‡æœ¬æ¸…æ´—æˆåŠŸç‡åº”å¤§äº95%");
        
        log.info("æ–‡æœ¬æ¸…æ´—æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {:.2f}%", successRate * 100);
    }

    /**
     * æµ‹è¯•2ï¼šå»é‡åŠŸèƒ½
     */
    @Test
    @Order(2)
    void testDeduplication() {
        log.info("å¼€å§‹æµ‹è¯•å»é‡åŠŸèƒ½...");
        
        // åˆ›å»ºåŒ…å«é‡å¤å†…å®¹çš„æµ‹è¯•æ•°æ®
        List<String> textsWithDuplicates = new ArrayList<>();
        textsWithDuplicates.addAll(testDataSet.stream()
            .map(ZhihuTestData::getRawContent)
            .collect(Collectors.toList()));
        
        // æ·»åŠ é‡å¤å†…å®¹
        textsWithDuplicates.addAll(testDataSet.subList(0, 10).stream()
            .map(ZhihuTestData::getRawContent)
            .collect(Collectors.toList()));
        
        int originalCount = textsWithDuplicates.size();
        
        // æ‰§è¡Œå»é‡
        Set<String> uniqueTexts = new HashSet<>();
        List<String> deduplicatedTexts = new ArrayList<>();
        
        for (String text : textsWithDuplicates) {
            String cleanedText = textProcessingService.cleanText(text);
            String textHash = Integer.toString(cleanedText.hashCode());
            
            if (!uniqueTexts.contains(textHash)) {
                uniqueTexts.add(textHash);
                deduplicatedTexts.add(cleanedText);
            }
        }
        
        int deduplicatedCount = deduplicatedTexts.size();
        double deduplicationRate = 1.0 - (double) deduplicatedCount / originalCount;
        
        DeduplicationResult result = DeduplicationResult.builder()
            .originalCount(originalCount)
            .deduplicatedCount(deduplicatedCount)
            .duplicatesRemoved(originalCount - deduplicatedCount)
            .deduplicationRate(deduplicationRate)
            .build();
        
        // ç”Ÿæˆå»é‡æŠ¥å‘Š
        generateDeduplicationReport(result);
        
        // éªŒè¯å»é‡æ•ˆæœ
        Assertions.assertTrue(deduplicationRate > 0, "åº”è¯¥æ£€æµ‹åˆ°é‡å¤å†…å®¹");
        Assertions.assertEquals(testDataSet.size(), deduplicatedCount, "å»é‡åæ•°é‡åº”è¯¥ç­‰äºåŸå§‹å”¯ä¸€æ•°æ®é‡");
        
        log.info("å»é‡æµ‹è¯•å®Œæˆï¼Œå»é‡ç‡: {:.2f}%", deduplicationRate * 100);
    }

    /**
     * æµ‹è¯•3ï¼šä¸­æ–‡åˆ†è¯åŠŸèƒ½
     */
    @Test
    @Order(3)
    void testTokenization() {
        log.info("å¼€å§‹æµ‹è¯•ä¸­æ–‡åˆ†è¯åŠŸèƒ½...");
        
        List<TokenizationTestResult> results = new ArrayList<>();
        
        for (ZhihuTestData data : testDataSet) {
            long startTime = System.currentTimeMillis();
            
            try {
                String cleanedText = textProcessingService.cleanText(data.getRawContent());
                TokenizationResult tokenResult = textProcessingService.tokenizeText(cleanedText);
                long processingTime = System.currentTimeMillis() - startTime;
                
                TokenizationTestResult result = TokenizationTestResult.builder()
                    .originalText(cleanedText)
                    .tokens(tokenResult.getTokens())
                    .filteredTokens(tokenResult.getFilteredTokens())
                    .posTagging(tokenResult.getPosTagging())
                    .tokenCount(tokenResult.getTokens().size())
                    .filteredTokenCount(tokenResult.getFilteredTokens().size())
                    .processingTimeMs(processingTime)
                    .averageTokenLength(calculateAverageTokenLength(tokenResult.getTokens()))
                    .chineseTokenRatio(calculateChineseTokenRatio(tokenResult.getTokens()))
                    .success(true)
                    .build();
                
                results.add(result);
                testMetrics.incrementSuccessCount();
                
            } catch (Exception e) {
                log.error("åˆ†è¯å¤„ç†å¤±è´¥: {}", e.getMessage());
                testMetrics.incrementFailureCount();
                
                results.add(TokenizationTestResult.builder()
                    .originalText(data.getRawContent())
                    .success(false)
                    .errorMessage(e.getMessage())
                    .build());
            }
        }
        
        // ç”Ÿæˆåˆ†è¯æµ‹è¯•æŠ¥å‘Š
        generateTokenizationReport(results);
        
        // éªŒè¯åˆ†è¯æ•ˆæœ
        double successRate = (double) testMetrics.getSuccessCount() / testDataSet.size();
        Assertions.assertTrue(successRate >= 0.95, "åˆ†è¯æˆåŠŸç‡åº”å¤§äº95%");
        
        // éªŒè¯ä¸­æ–‡åˆ†è¯è´¨é‡
        double avgChineseRatio = results.stream()
            .filter(TokenizationTestResult::isSuccess)
            .mapToDouble(TokenizationTestResult::getChineseTokenRatio)
            .average()
            .orElse(0.0);
        
        Assertions.assertTrue(avgChineseRatio >= 0.7, "ä¸­æ–‡è¯æ±‡æ¯”ä¾‹åº”å¤§äº70%");
        
        log.info("åˆ†è¯æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {:.2f}%, ä¸­æ–‡è¯æ±‡æ¯”ä¾‹: {:.2f}%", 
            successRate * 100, avgChineseRatio * 100);
    }

    /**
     * æµ‹è¯•4ï¼šTF-IDFå‘é‡åŒ–åŠŸèƒ½
     */
    @Test
    @Order(4)
    void testTfIdfVectorization() {
        log.info("å¼€å§‹æµ‹è¯•TF-IDFå‘é‡åŒ–åŠŸèƒ½...");
        
        List<TfIdfTestResult> results = new ArrayList<>();
        List<String> corpus = new ArrayList<>();
        
        // å‡†å¤‡è¯­æ–™åº“
        for (ZhihuTestData data : testDataSet) {
            String cleanedText = textProcessingService.cleanText(data.getRawContent());
            TokenizationResult tokenResult = textProcessingService.tokenizeText(cleanedText);
            corpus.add(String.join(" ", tokenResult.getFilteredTokens()));
        }
        
        for (int i = 0; i < corpus.size(); i++) {
            long startTime = System.currentTimeMillis();
            
            try {
                String text = corpus.get(i);
                TokenizationResult tokenResult = textProcessingService.tokenizeText(text);
                TextFeatures features = featureExtractionService.extractFeatures(text, tokenResult);
                long processingTime = System.currentTimeMillis() - startTime;
                
                TfIdfTestResult result = TfIdfTestResult.builder()
                    .documentIndex(i)
                    .originalText(text)
                    .features(features)
                    .processingTimeMs(processingTime)
                    .vectorDimension(features.getTfidfVector() != null ? features.getTfidfVector().size() : 0)
                    .nonZeroFeatures(countNonZeroFeatures(features.getTfidfVector()))
                    .maxTfIdfValue(getMaxTfIdfValue(features.getTfidfVector()))
                    .success(true)
                    .build();
                
                results.add(result);
                testMetrics.incrementSuccessCount();
                
            } catch (Exception e) {
                log.error("TF-IDFå‘é‡åŒ–å¤±è´¥: {}", e.getMessage());
                testMetrics.incrementFailureCount();
                
                results.add(TfIdfTestResult.builder()
                    .documentIndex(i)
                    .success(false)
                    .errorMessage(e.getMessage())
                    .build());
            }
        }
        
        // ç”ŸæˆTF-IDFæµ‹è¯•æŠ¥å‘Š
        generateTfIdfReport(results);
        
        // éªŒè¯å‘é‡åŒ–æ•ˆæœ
        double successRate = (double) testMetrics.getSuccessCount() / testDataSet.size();
        Assertions.assertTrue(successRate >= 0.90, "TF-IDFå‘é‡åŒ–æˆåŠŸç‡åº”å¤§äº90%");
        
        // éªŒè¯å‘é‡è´¨é‡
        List<TfIdfTestResult> successResults = results.stream()
            .filter(TfIdfTestResult::isSuccess)
            .collect(Collectors.toList());
        
        double avgVectorDimension = successResults.stream()
            .mapToDouble(TfIdfTestResult::getVectorDimension)
            .average()
            .orElse(0.0);
        
        Assertions.assertTrue(avgVectorDimension > 0, "å‘é‡ç»´åº¦åº”å¤§äº0");
        
        log.info("TF-IDFå‘é‡åŒ–æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {:.2f}%, å¹³å‡å‘é‡ç»´åº¦: {:.0f}", 
            successRate * 100, avgVectorDimension);
    }

    /**
     * æµ‹è¯•5ï¼šç‰¹å¾å·¥ç¨‹ç»¼åˆæµ‹è¯•
     */
    @Test
    @Order(5)
    void testFeatureEngineering() {
        log.info("å¼€å§‹æµ‹è¯•ç‰¹å¾å·¥ç¨‹ç»¼åˆåŠŸèƒ½...");
        
        List<FeatureEngineeringResult> results = new ArrayList<>();
        
        for (ZhihuTestData data : testDataSet) {
            long startTime = System.currentTimeMillis();
            
            try {
                ProcessingResult processingResult = textProcessingService.processText(
                    data.getRawContent(), "zhihu", data.getMetadata());
                long processingTime = System.currentTimeMillis() - startTime;
                
                FeatureEngineeringResult result = FeatureEngineeringResult.builder()
                    .originalText(data.getRawContent())
                    .processingResult(processingResult)
                    .processingTimeMs(processingTime)
                    .textLength(processingResult.getCleanedText().length())
                    .tokenCount(processingResult.getTokenizationResult().getTokens().size())
                    .featureCount(processingResult.getFeatures() != null ? 
                        countTotalFeatures(processingResult.getFeatures()) : 0)
                    .success(processingResult.isSuccess())
                    .build();
                
                results.add(result);
                
                if (processingResult.isSuccess()) {
                    testMetrics.incrementSuccessCount();
                } else {
                    testMetrics.incrementFailureCount();
                }
                
            } catch (Exception e) {
                log.error("ç‰¹å¾å·¥ç¨‹å¤„ç†å¤±è´¥: {}", e.getMessage());
                testMetrics.incrementFailureCount();
                
                results.add(FeatureEngineeringResult.builder()
                    .originalText(data.getRawContent())
                    .success(false)
                    .errorMessage(e.getMessage())
                    .build());
            }
        }
        
        // ç”Ÿæˆç‰¹å¾å·¥ç¨‹æµ‹è¯•æŠ¥å‘Š
        generateFeatureEngineeringReport(results);
        
        // éªŒè¯ç‰¹å¾å·¥ç¨‹æ•ˆæœ
        double successRate = (double) testMetrics.getSuccessCount() / testDataSet.size();
        Assertions.assertTrue(successRate >= 0.90, "ç‰¹å¾å·¥ç¨‹æˆåŠŸç‡åº”å¤§äº90%");
        
        log.info("ç‰¹å¾å·¥ç¨‹æµ‹è¯•å®Œæˆï¼ŒæˆåŠŸç‡: {:.2f}%", successRate * 100);
    }

    /**
     * æµ‹è¯•6ï¼šæ€§èƒ½å‹åŠ›æµ‹è¯•
     */
    @Test
    @Order(6)
    void testPerformanceStress() {
        log.info("å¼€å§‹æ€§èƒ½å‹åŠ›æµ‹è¯•...");
        
        int threadCount = 10;
        int batchSize = 100;
        ExecutorService executor = Executors.newFixedThreadPool(threadCount);
        
        List<CompletableFuture<PerformanceResult>> futures = new ArrayList<>();
        
        for (int i = 0; i < threadCount; i++) {
            final int threadId = i;
            CompletableFuture<PerformanceResult> future = CompletableFuture.supplyAsync(() -> {
                return executePerformanceTest(threadId, batchSize);
            }, executor);
            futures.add(future);
        }
        
        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        List<PerformanceResult> results = futures.stream()
            .map(CompletableFuture::join)
            .collect(Collectors.toList());
        
        executor.shutdown();
        
        // ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š
        generatePerformanceReport(results);
        
        // éªŒè¯æ€§èƒ½æŒ‡æ ‡
        double avgThroughput = results.stream()
            .mapToDouble(PerformanceResult::getThroughput)
            .average()
            .orElse(0.0);
        
        double avgLatency = results.stream()
            .mapToDouble(PerformanceResult::getAverageLatency)
            .average()
            .orElse(0.0);
        
        Assertions.assertTrue(avgThroughput > 10, "å¹³å‡ååé‡åº”å¤§äº10 TPS");
        Assertions.assertTrue(avgLatency < 1000, "å¹³å‡å»¶è¿Ÿåº”å°äº1000ms");
        
        log.info("æ€§èƒ½å‹åŠ›æµ‹è¯•å®Œæˆï¼Œå¹³å‡ååé‡: {:.2f} TPS, å¹³å‡å»¶è¿Ÿ: {:.2f} ms", 
            avgThroughput, avgLatency);
    }

    @AfterAll
    static void generateFinalReport() throws IOException {
        log.info("ç”Ÿæˆæœ€ç»ˆæµ‹è¯•æŠ¥å‘Š...");
        
        String reportPath = REPORT_DIR + "/final-report-" + 
            LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd-HHmmss")) + ".md";
        
        try (FileWriter writer = new FileWriter(reportPath)) {
            writer.write("# æ•°æ®é¢„å¤„ç†æœåŠ¡ç”Ÿäº§çº§æµ‹è¯•æŠ¥å‘Š\n\n");
            writer.write("## æµ‹è¯•æ¦‚è¿°\n");
            writer.write("æœ¬æŠ¥å‘ŠåŒ…å«äº†å¯¹æ•°æ®é¢„å¤„ç†æœåŠ¡çš„å…¨é¢æµ‹è¯•ç»“æœï¼Œæ¶µç›–æ–‡æœ¬æ¸…æ´—ã€å»é‡ã€åˆ†è¯ã€TF-IDFå‘é‡åŒ–ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚\n\n");
            writer.write("## æµ‹è¯•ç¯å¢ƒ\n");
            writer.write("- æµ‹è¯•æ—¶é—´: " + LocalDateTime.now() + "\n");
            writer.write("- æµ‹è¯•æ•°æ®: çŸ¥ä¹è¯é¢˜è¯„è®ºæ•°æ®\n");
            writer.write("- æµ‹è¯•æ¡†æ¶: JUnit 5 + Spring Boot Test\n\n");
            writer.write("## è¯¦ç»†æŠ¥å‘Š\n");
            writer.write("è¯·æŸ¥çœ‹å„ä¸ªåŠŸèƒ½æ¨¡å—çš„è¯¦ç»†æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶ã€‚\n");
        }
        
        log.info("æœ€ç»ˆæµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {}", reportPath);
    }

    // è¾…åŠ©æ–¹æ³•å’Œæ•°æ®ç±»å®šä¹‰
    
    private List<ZhihuTestData> generateZhihuTestData() {
        List<ZhihuTestData> data = new ArrayList<>();
        
        // æ¨¡æ‹ŸçŸ¥ä¹è¯„è®ºæ•°æ®
        String[] sampleTexts = {
            "è¿™ä¸ªè§‚ç‚¹å¾ˆæœ‰é“ç†ï¼Œæˆ‘è§‰å¾—<b>äººå·¥æ™ºèƒ½</b>ç¡®å®ä¼šæ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚https://www.zhihu.com/question/123456",
            "åŒæ„æ¥¼ä¸Šçš„çœ‹æ³•ï¼ï¼ï¼ç°åœ¨çš„AIæŠ€æœ¯å‘å±•å¤ªå¿«äº†ï¼Œç‰¹åˆ«æ˜¯ChatGPTçš„å‡ºç° ğŸ“±ğŸ’»",
            "<p>ä¸è¿‡æˆ‘è§‰å¾—è¿˜æ˜¯è¦ç†æ€§çœ‹å¾…ï¼ŒæŠ€æœ¯å‘å±•éœ€è¦æ—¶é—´ã€‚è”ç³»æ–¹å¼ï¼šexample@email.com</p>",
            "å“ˆå“ˆå“ˆå“ˆï¼Œè¿™ä¸ªå›ç­”å¤ªæç¬‘äº†ğŸ˜‚ğŸ˜‚ğŸ˜‚ ç”µè¯ï¼š138-0013-8000",
            "ä»æŠ€æœ¯è§’åº¦æ¥è¯´ï¼Œæ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ ç¡®å®æœ‰å¾ˆå¤§çš„åº”ç”¨å‰æ™¯...",
            "   \n\n   ç©ºç™½å†…å®¹æµ‹è¯•   \n\n   ",
            "é‡å¤å†…å®¹æµ‹è¯•é‡å¤å†…å®¹æµ‹è¯•é‡å¤å†…å®¹æµ‹è¯•",
            "ä¸­è‹±æ–‡æ··åˆæµ‹è¯• English mixed content æµ‹è¯•å†…å®¹",
            "ç‰¹æ®Šå­—ç¬¦æµ‹è¯•ï¼@#$%^&*()_+-=[]{}|;':\",./<>?",
            "é•¿æ–‡æœ¬æµ‹è¯•ï¼š" + "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬å†…å®¹ï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿå¯¹é•¿æ–‡æœ¬çš„å¤„ç†èƒ½åŠ›ã€‚".repeat(10)
        };
        
        for (int i = 0; i < sampleTexts.length; i++) {
            Map<String, Object> metadata = new HashMap<>();
            metadata.put("questionId", "Q" + (i + 1));
            metadata.put("answerId", "A" + (i + 1));
            metadata.put("userId", "U" + (i + 1));
            metadata.put("timestamp", System.currentTimeMillis());
            
            data.add(ZhihuTestData.builder()
                .id("zhihu_" + i)
                .rawContent(sampleTexts[i])
                .source("zhihu")
                .metadata(metadata)
                .build());
        }
        
        return data;
    }

    // å…¶ä»–è¾…åŠ©æ–¹æ³•...
    private int countHtmlTags(String text) {
        return text.split("<[^>]+>").length - 1;
    }

    private int countUrls(String text) {
        return text.split("https?://[\\w\\.-]+").length - 1;
    }

    private int countSpecialChars(String text) {
        return (int) text.chars().filter(c -> !Character.isLetterOrDigit(c) && !Character.isWhitespace(c)).count();
    }

    private double calculateAverageTokenLength(List<String> tokens) {
        return tokens.stream().mapToInt(String::length).average().orElse(0.0);
    }

    private double calculateChineseTokenRatio(List<String> tokens) {
        long chineseTokens = tokens.stream()
            .filter(token -> token.chars().anyMatch(c -> Character.UnicodeScript.of(c) == Character.UnicodeScript.HAN))
            .count();
        return tokens.isEmpty() ? 0.0 : (double) chineseTokens / tokens.size();
    }

    private int countNonZeroFeatures(Map<String, Double> vector) {
        return vector == null ? 0 : (int) vector.values().stream().filter(v -> v != 0.0).count();
    }

    private double getMaxTfIdfValue(Map<String, Double> vector) {
        return vector == null ? 0.0 : vector.values().stream().mapToDouble(Double::doubleValue).max().orElse(0.0);
    }

    private int countTotalFeatures(TextFeatures features) {
        int count = 0;
        if (features.getTfidfVector() != null) count += features.getTfidfVector().size();
        if (features.getStatistical() != null) count += 1;
        if (features.getWord2vecVector() != null) count += features.getWord2vecVector().size();
        if (features.getNgramFeatures() != null) count += features.getNgramFeatures().size();
        if (features.getSentiment() != null) count += 1;
        if (features.getLanguage() != null) count += 1;
        return count;
    }

    private PerformanceResult executePerformanceTest(int threadId, int batchSize) {
        long startTime = System.currentTimeMillis();
        int successCount = 0;
        int failureCount = 0;
        List<Long> latencies = new ArrayList<>();
        
        for (int i = 0; i < batchSize; i++) {
            ZhihuTestData data = testDataSet.get(i % testDataSet.size());
            long requestStart = System.currentTimeMillis();
            
            try {
                textProcessingService.processText(data.getRawContent(), "zhihu", data.getMetadata());
                successCount++;
            } catch (Exception e) {
                failureCount++;
            }
            
            latencies.add(System.currentTimeMillis() - requestStart);
        }
        
        long totalTime = System.currentTimeMillis() - startTime;
        double throughput = (double) batchSize / (totalTime / 1000.0);
        double averageLatency = latencies.stream().mapToLong(Long::longValue).average().orElse(0.0);
        
        return PerformanceResult.builder()
            .threadId(threadId)
            .totalRequests(batchSize)
            .successCount(successCount)
            .failureCount(failureCount)
            .totalTimeMs(totalTime)
            .throughput(throughput)
            .averageLatency(averageLatency)
            .build();
    }

    // æŠ¥å‘Šç”Ÿæˆæ–¹æ³•
    private void generateCleaningReport(List<CleaningTestResult> results) {
        // å®ç°æ¸…æ´—æµ‹è¯•æŠ¥å‘Šç”Ÿæˆé€»è¾‘
    }

    private void generateDeduplicationReport(DeduplicationResult result) {
        // å®ç°å»é‡æµ‹è¯•æŠ¥å‘Šç”Ÿæˆé€»è¾‘
    }

    private void generateTokenizationReport(List<TokenizationTestResult> results) {
        // å®ç°åˆ†è¯æµ‹è¯•æŠ¥å‘Šç”Ÿæˆé€»è¾‘
    }

    private void generateTfIdfReport(List<TfIdfTestResult> results) {
        // å®ç°TF-IDFæµ‹è¯•æŠ¥å‘Šç”Ÿæˆé€»è¾‘
    }

    private void generateFeatureEngineeringReport(List<FeatureEngineeringResult> results) {
        // å®ç°ç‰¹å¾å·¥ç¨‹æµ‹è¯•æŠ¥å‘Šç”Ÿæˆé€»è¾‘
    }

    private void generatePerformanceReport(List<PerformanceResult> results) {
        // å®ç°æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆé€»è¾‘
    }

    // æ•°æ®ç±»å®šä¹‰
    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class ZhihuTestData {
        private String id;
        private String rawContent;
        private String source;
        private Map<String, Object> metadata;
    }

    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class TestMetrics {
        private int successCount;
        private int failureCount;
        
        public void incrementSuccessCount() { successCount++; }
        public void incrementFailureCount() { failureCount++; }
    }

    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class CleaningTestResult {
        private String originalText;
        private String cleanedText;
        private int originalLength;
        private int cleanedLength;
        private long processingTimeMs;
        private int htmlTagsRemoved;
        private int urlsRemoved;
        private int specialCharsRemoved;
        private boolean success;
        private String errorMessage;
    }

    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class DeduplicationResult {
        private int originalCount;
        private int deduplicatedCount;
        private int duplicatesRemoved;
        private double deduplicationRate;
    }

    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class TokenizationTestResult {
        private String originalText;
        private List<String> tokens;
        private List<String> filteredTokens;
        private Map<String, String> posTagging;
        private int tokenCount;
        private int filteredTokenCount;
        private long processingTimeMs;
        private double averageTokenLength;
        private double chineseTokenRatio;
        private boolean success;
        private String errorMessage;
    }

    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class TfIdfTestResult {
        private int documentIndex;
        private String originalText;
        private TextFeatures features;
        private long processingTimeMs;
        private int vectorDimension;
        private int nonZeroFeatures;
        private double maxTfIdfValue;
        private boolean success;
        private String errorMessage;
    }

    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class FeatureEngineeringResult {
        private String originalText;
        private ProcessingResult processingResult;
        private long processingTimeMs;
        private int textLength;
        private int tokenCount;
        private int featureCount;
        private boolean success;
        private String errorMessage;
    }

    @lombok.Data
    @lombok.Builder
    @lombok.NoArgsConstructor
    @lombok.AllArgsConstructor
    static class PerformanceResult {
        private int threadId;
        private int totalRequests;
        private int successCount;
        private int failureCount;
        private long totalTimeMs;
        private double throughput;
        private double averageLatency;
    }
}